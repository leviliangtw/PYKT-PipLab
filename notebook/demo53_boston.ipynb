{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                896       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,121\n",
      "Trainable params: 5,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 363 samples, validate on 41 samples\n",
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 409us/step - loss: 491.8558 - mae: 20.0926 - val_loss: 304.1548 - val_mae: 16.1055\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 177us/step - loss: 259.8130 - mae: 13.5692 - val_loss: 99.4904 - val_mae: 8.5623\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 98us/step - loss: 83.2762 - mae: 6.8901 - val_loss: 31.3040 - val_mae: 4.8195\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 94us/step - loss: 40.9593 - mae: 4.5662 - val_loss: 18.0931 - val_mae: 3.5822\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 204us/step - loss: 29.1595 - mae: 3.7435 - val_loss: 14.8493 - val_mae: 3.0929\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 268us/step - loss: 24.1186 - mae: 3.3943 - val_loss: 14.6451 - val_mae: 3.0591\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 226us/step - loss: 20.6593 - mae: 3.0897 - val_loss: 17.0844 - val_mae: 3.3351\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 214us/step - loss: 19.2137 - mae: 3.0168 - val_loss: 13.8513 - val_mae: 2.9662\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 165us/step - loss: 17.3636 - mae: 2.7989 - val_loss: 12.0874 - val_mae: 2.7279\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 116us/step - loss: 15.6257 - mae: 2.6979 - val_loss: 11.6149 - val_mae: 2.5941\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 124us/step - loss: 14.9202 - mae: 2.5932 - val_loss: 12.9504 - val_mae: 2.7767\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 178us/step - loss: 13.5570 - mae: 2.5241 - val_loss: 9.9311 - val_mae: 2.4432\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 314us/step - loss: 13.0486 - mae: 2.4786 - val_loss: 11.8931 - val_mae: 2.8846\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 179us/step - loss: 12.6010 - mae: 2.4340 - val_loss: 10.4458 - val_mae: 2.7260\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 222us/step - loss: 12.2516 - mae: 2.3750 - val_loss: 9.0060 - val_mae: 2.4687\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 177us/step - loss: 11.7070 - mae: 2.3788 - val_loss: 8.6904 - val_mae: 2.4192\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 213us/step - loss: 11.4687 - mae: 2.2749 - val_loss: 11.0981 - val_mae: 2.7473\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 235us/step - loss: 11.2187 - mae: 2.3030 - val_loss: 9.1297 - val_mae: 2.4609\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 221us/step - loss: 10.9150 - mae: 2.2624 - val_loss: 8.6088 - val_mae: 2.3692\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 217us/step - loss: 10.5768 - mae: 2.2474 - val_loss: 8.5945 - val_mae: 2.3609\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 203us/step - loss: 10.4607 - mae: 2.2158 - val_loss: 8.1989 - val_mae: 2.2720\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 204us/step - loss: 10.1076 - mae: 2.2054 - val_loss: 7.9638 - val_mae: 2.2961\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 187us/step - loss: 9.8946 - mae: 2.1635 - val_loss: 8.6805 - val_mae: 2.2773\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 150us/step - loss: 9.9010 - mae: 2.1709 - val_loss: 8.5656 - val_mae: 2.3419\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 145us/step - loss: 9.7639 - mae: 2.1072 - val_loss: 7.6090 - val_mae: 2.2209\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 213us/step - loss: 9.3824 - mae: 2.0937 - val_loss: 8.5712 - val_mae: 2.4271\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 194us/step - loss: 9.2649 - mae: 2.1142 - val_loss: 7.7710 - val_mae: 2.2286\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 166us/step - loss: 9.3847 - mae: 2.1190 - val_loss: 7.8614 - val_mae: 2.3573\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 195us/step - loss: 9.0778 - mae: 2.0532 - val_loss: 7.7416 - val_mae: 2.2010\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 186us/step - loss: 9.2686 - mae: 2.0795 - val_loss: 7.1169 - val_mae: 2.1618\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 215us/step - loss: 8.9419 - mae: 2.0385 - val_loss: 7.7147 - val_mae: 2.2886\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 191us/step - loss: 8.7579 - mae: 2.0290 - val_loss: 6.7306 - val_mae: 2.1228\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 218us/step - loss: 8.6125 - mae: 2.0152 - val_loss: 8.3648 - val_mae: 2.5395\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 202us/step - loss: 8.7006 - mae: 2.0228 - val_loss: 7.1702 - val_mae: 2.0605\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 142us/step - loss: 8.4848 - mae: 2.0328 - val_loss: 8.3190 - val_mae: 2.4867\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 101us/step - loss: 8.5048 - mae: 1.9911 - val_loss: 8.3602 - val_mae: 2.4782\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 106us/step - loss: 8.3344 - mae: 1.9836 - val_loss: 7.2288 - val_mae: 2.2438\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 142us/step - loss: 8.3357 - mae: 1.9518 - val_loss: 7.6948 - val_mae: 2.2381\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 103us/step - loss: 8.2428 - mae: 2.0046 - val_loss: 7.7529 - val_mae: 2.3216\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 136us/step - loss: 8.4086 - mae: 1.9590 - val_loss: 6.4793 - val_mae: 2.0070\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 169us/step - loss: 7.8475 - mae: 1.9377 - val_loss: 8.7668 - val_mae: 2.5184\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 138us/step - loss: 8.1533 - mae: 1.9398 - val_loss: 6.8005 - val_mae: 2.0695\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 143us/step - loss: 7.7213 - mae: 1.9174 - val_loss: 6.8289 - val_mae: 2.0592\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 110us/step - loss: 7.6532 - mae: 1.8837 - val_loss: 7.4216 - val_mae: 2.1438\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 109us/step - loss: 7.5883 - mae: 1.8777 - val_loss: 7.0449 - val_mae: 2.1221\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 137us/step - loss: 7.7541 - mae: 1.8979 - val_loss: 7.2593 - val_mae: 2.2479\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 134us/step - loss: 7.5501 - mae: 1.8786 - val_loss: 6.4756 - val_mae: 2.0555\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 144us/step - loss: 7.3532 - mae: 1.8817 - val_loss: 7.8052 - val_mae: 2.3732\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 155us/step - loss: 7.6496 - mae: 1.8582 - val_loss: 6.6937 - val_mae: 2.1815\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 108us/step - loss: 7.5638 - mae: 1.8805 - val_loss: 7.4996 - val_mae: 2.2680\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 157us/step - loss: 7.3225 - mae: 1.8198 - val_loss: 7.0736 - val_mae: 2.2191\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 148us/step - loss: 7.4648 - mae: 1.8874 - val_loss: 6.8835 - val_mae: 2.1813\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 149us/step - loss: 7.3425 - mae: 1.8242 - val_loss: 6.6657 - val_mae: 2.1310\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 131us/step - loss: 7.2655 - mae: 1.8560 - val_loss: 7.2151 - val_mae: 2.2341\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 137us/step - loss: 7.0225 - mae: 1.8126 - val_loss: 8.2744 - val_mae: 2.4238\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 125us/step - loss: 7.0843 - mae: 1.7947 - val_loss: 6.3967 - val_mae: 1.9998\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 107us/step - loss: 7.2336 - mae: 1.8302 - val_loss: 7.2654 - val_mae: 2.2593\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 93us/step - loss: 7.0952 - mae: 1.8192 - val_loss: 6.2209 - val_mae: 2.0806\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 122us/step - loss: 6.8481 - mae: 1.7848 - val_loss: 5.6646 - val_mae: 1.9290\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 123us/step - loss: 6.8444 - mae: 1.8047 - val_loss: 6.3781 - val_mae: 2.0957\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 115us/step - loss: 7.0591 - mae: 1.8322 - val_loss: 6.4412 - val_mae: 2.1456\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 130us/step - loss: 6.6410 - mae: 1.7425 - val_loss: 5.9773 - val_mae: 1.9926\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 129us/step - loss: 6.7762 - mae: 1.7622 - val_loss: 5.5998 - val_mae: 1.8702\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 176us/step - loss: 6.7183 - mae: 1.7814 - val_loss: 6.6190 - val_mae: 2.1245\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 108us/step - loss: 6.5224 - mae: 1.7248 - val_loss: 6.2873 - val_mae: 1.8878\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 111us/step - loss: 6.4468 - mae: 1.7010 - val_loss: 8.9649 - val_mae: 2.5276\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 130us/step - loss: 6.5972 - mae: 1.7570 - val_loss: 5.7318 - val_mae: 1.9330\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 126us/step - loss: 6.5726 - mae: 1.7964 - val_loss: 6.2964 - val_mae: 2.1119\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 114us/step - loss: 6.3924 - mae: 1.7194 - val_loss: 7.0874 - val_mae: 2.2347\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 96us/step - loss: 6.2566 - mae: 1.7608 - val_loss: 7.2413 - val_mae: 2.2846\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 118us/step - loss: 6.4630 - mae: 1.7506 - val_loss: 8.0633 - val_mae: 2.4102\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 132us/step - loss: 6.1069 - mae: 1.7015 - val_loss: 6.6448 - val_mae: 2.1133\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 181us/step - loss: 6.4374 - mae: 1.7305 - val_loss: 5.7119 - val_mae: 1.9627\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 194us/step - loss: 6.1428 - mae: 1.7162 - val_loss: 7.5324 - val_mae: 2.3565\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 123us/step - loss: 6.2804 - mae: 1.7081 - val_loss: 5.9204 - val_mae: 2.0415\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 123us/step - loss: 6.0088 - mae: 1.6793 - val_loss: 5.7473 - val_mae: 1.8555\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 128us/step - loss: 5.8292 - mae: 1.6582 - val_loss: 5.9563 - val_mae: 1.9887\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 102us/step - loss: 6.2094 - mae: 1.6871 - val_loss: 6.6732 - val_mae: 2.1251\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 128us/step - loss: 5.8085 - mae: 1.6737 - val_loss: 6.4607 - val_mae: 2.1076\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 103us/step - loss: 5.8496 - mae: 1.6414 - val_loss: 5.7493 - val_mae: 1.9009\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 100us/step - loss: 6.0231 - mae: 1.6599 - val_loss: 6.8203 - val_mae: 2.1398\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 103us/step - loss: 5.9356 - mae: 1.6619 - val_loss: 5.7680 - val_mae: 1.8860\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 135us/step - loss: 5.6755 - mae: 1.6417 - val_loss: 5.8146 - val_mae: 1.9766\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 130us/step - loss: 5.7761 - mae: 1.6290 - val_loss: 5.6357 - val_mae: 1.9118\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 140us/step - loss: 5.6182 - mae: 1.6197 - val_loss: 8.1166 - val_mae: 2.3562\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 94us/step - loss: 5.5941 - mae: 1.6522 - val_loss: 7.2033 - val_mae: 2.2457\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 135us/step - loss: 5.8119 - mae: 1.6560 - val_loss: 6.5566 - val_mae: 2.0960\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 137us/step - loss: 5.6784 - mae: 1.6624 - val_loss: 5.3093 - val_mae: 1.8673\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 212us/step - loss: 5.4977 - mae: 1.6190 - val_loss: 5.8994 - val_mae: 1.9804\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 143us/step - loss: 5.3589 - mae: 1.5911 - val_loss: 5.2590 - val_mae: 1.8478\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 85us/step - loss: 5.3945 - mae: 1.5659 - val_loss: 6.4081 - val_mae: 1.9678\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 109us/step - loss: 5.5280 - mae: 1.5983 - val_loss: 7.2444 - val_mae: 2.2587\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 130us/step - loss: 5.3697 - mae: 1.5749 - val_loss: 5.5165 - val_mae: 1.8738\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 128us/step - loss: 5.4532 - mae: 1.6116 - val_loss: 5.5028 - val_mae: 1.9161\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 171us/step - loss: 4.9585 - mae: 1.5152 - val_loss: 9.3471 - val_mae: 2.5666\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 89us/step - loss: 5.2754 - mae: 1.5709 - val_loss: 6.3342 - val_mae: 1.9772\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 122us/step - loss: 5.1021 - mae: 1.5198 - val_loss: 6.7875 - val_mae: 2.0202\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 114us/step - loss: 5.1544 - mae: 1.5364 - val_loss: 5.5680 - val_mae: 1.8515\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 117us/step - loss: 4.9720 - mae: 1.5190 - val_loss: 6.8438 - val_mae: 2.0330\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 86us/step - loss: 5.2661 - mae: 1.5554 - val_loss: 6.6416 - val_mae: 2.1025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fdf48465a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "# print(test_data.shape)\n",
    "# print(test_labels.shape)\n",
    "\n",
    "# print(train_data[0])\n",
    "# print(train_labels[0])\n",
    "# print(test_data[0])\n",
    "# print(test_labels[0])\n",
    "# print(\"Breakpoint!\")\n",
    "\n",
    "# Make the Z table: mean=0, std=1\n",
    "mean = train_data.mean(axis=0)  # y-axis, column\n",
    "std = train_data.std(axis=0)  # y-axis, column\n",
    "train_data -= mean\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std\n",
    "\n",
    "\n",
    "# print(\"Breakpoint!, mean: \",mean)\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "# print(train_data.shape[1], 64*14)\n",
    "model.fit(train_data, train_labels, validation_split=0.1,\n",
    "          epochs=100, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_label:7.20, test_predict:7.74, RMAE:6.74%\n",
      "\n",
      "test_label:18.80, test_predict:18.47, RMAE:17.47%\n",
      "\n",
      "test_label:19.00, test_predict:20.54, RMAE:19.54%\n",
      "\n",
      "test_label:27.00, test_predict:32.51, RMAE:31.51%\n",
      "\n",
      "test_label:22.20, test_predict:24.21, RMAE:23.21%\n",
      "\n",
      "test_label:24.50, test_predict:21.99, RMAE:20.99%\n",
      "\n",
      "test_label:31.20, test_predict:27.70, RMAE:26.70%\n",
      "\n",
      "test_label:22.90, test_predict:21.14, RMAE:20.14%\n",
      "\n",
      "test_label:20.50, test_predict:17.91, RMAE:16.91%\n",
      "\n",
      "test_label:23.20, test_predict:20.94, RMAE:19.94%\n",
      "\n",
      "test_label:18.60, test_predict:21.29, RMAE:20.29%\n",
      "\n",
      "test_label:14.50, test_predict:16.14, RMAE:15.14%\n",
      "\n",
      "test_label:17.80, test_predict:14.10, RMAE:13.10%\n",
      "\n",
      "test_label:50.00, test_predict:40.40, RMAE:39.40%\n",
      "\n",
      "test_label:20.80, test_predict:20.57, RMAE:19.57%\n",
      "\n",
      "test_label:24.30, test_predict:19.99, RMAE:18.99%\n",
      "\n",
      "test_label:24.20, test_predict:24.56, RMAE:23.56%\n",
      "\n",
      "test_label:19.80, test_predict:18.67, RMAE:17.67%\n",
      "\n",
      "test_label:19.10, test_predict:17.75, RMAE:16.75%\n",
      "\n",
      "test_label:22.70, test_predict:25.58, RMAE:24.58%\n",
      "\n",
      "test_label:12.00, test_predict:11.13, RMAE:10.13%\n",
      "\n",
      "test_label:10.20, test_predict:14.03, RMAE:13.03%\n",
      "\n",
      "test_label:20.00, test_predict:20.45, RMAE:19.45%\n",
      "\n",
      "test_label:18.50, test_predict:14.22, RMAE:13.22%\n",
      "\n",
      "test_label:20.90, test_predict:19.00, RMAE:18.00%\n",
      "\n",
      "test_label:23.00, test_predict:24.30, RMAE:23.30%\n",
      "\n",
      "test_label:27.50, test_predict:28.68, RMAE:27.68%\n",
      "\n",
      "test_label:30.10, test_predict:28.34, RMAE:27.34%\n",
      "\n",
      "test_label:9.50, test_predict:9.85, RMAE:8.85%\n",
      "\n",
      "test_label:22.00, test_predict:19.24, RMAE:18.24%\n",
      "\n",
      "test_label:21.20, test_predict:18.94, RMAE:17.94%\n",
      "\n",
      "test_label:14.10, test_predict:14.02, RMAE:13.02%\n",
      "\n",
      "test_label:33.10, test_predict:31.51, RMAE:30.51%\n",
      "\n",
      "test_label:23.40, test_predict:22.91, RMAE:21.91%\n",
      "\n",
      "test_label:20.10, test_predict:18.61, RMAE:17.61%\n",
      "\n",
      "test_label:7.40, test_predict:7.59, RMAE:6.59%\n",
      "\n",
      "test_label:15.40, test_predict:15.27, RMAE:14.27%\n",
      "\n",
      "test_label:23.80, test_predict:17.83, RMAE:16.83%\n",
      "\n",
      "test_label:20.10, test_predict:19.21, RMAE:18.21%\n",
      "\n",
      "test_label:24.50, test_predict:25.53, RMAE:24.53%\n",
      "\n",
      "test_label:33.00, test_predict:31.51, RMAE:30.51%\n",
      "\n",
      "test_label:28.40, test_predict:26.61, RMAE:25.61%\n",
      "\n",
      "test_label:14.10, test_predict:13.17, RMAE:12.17%\n",
      "\n",
      "test_label:46.70, test_predict:42.78, RMAE:41.78%\n",
      "\n",
      "test_label:32.50, test_predict:28.72, RMAE:27.72%\n",
      "\n",
      "test_label:29.60, test_predict:24.98, RMAE:23.98%\n",
      "\n",
      "test_label:28.40, test_predict:26.33, RMAE:25.33%\n",
      "\n",
      "test_label:19.80, test_predict:16.68, RMAE:15.68%\n",
      "\n",
      "test_label:20.20, test_predict:23.90, RMAE:22.90%\n",
      "\n",
      "test_label:25.00, test_predict:21.51, RMAE:20.51%\n",
      "\n",
      "test_label:35.40, test_predict:34.81, RMAE:33.81%\n",
      "\n",
      "test_label:20.30, test_predict:18.33, RMAE:17.33%\n",
      "\n",
      "test_label:9.70, test_predict:9.83, RMAE:8.83%\n",
      "\n",
      "test_label:14.50, test_predict:13.26, RMAE:12.26%\n",
      "\n",
      "test_label:34.90, test_predict:35.56, RMAE:34.56%\n",
      "\n",
      "test_label:26.60, test_predict:26.86, RMAE:25.86%\n",
      "\n",
      "test_label:7.20, test_predict:12.15, RMAE:11.15%\n",
      "\n",
      "test_label:50.00, test_predict:47.72, RMAE:46.72%\n",
      "\n",
      "test_label:32.40, test_predict:32.80, RMAE:31.80%\n",
      "\n",
      "test_label:21.60, test_predict:23.16, RMAE:22.16%\n",
      "\n",
      "test_label:29.80, test_predict:24.17, RMAE:23.17%\n",
      "\n",
      "test_label:13.10, test_predict:16.54, RMAE:15.54%\n",
      "\n",
      "test_label:27.50, test_predict:14.83, RMAE:13.83%\n",
      "\n",
      "test_label:21.20, test_predict:19.32, RMAE:18.32%\n",
      "\n",
      "test_label:23.10, test_predict:22.32, RMAE:21.32%\n",
      "\n",
      "test_label:21.90, test_predict:21.08, RMAE:20.08%\n",
      "\n",
      "test_label:13.00, test_predict:12.81, RMAE:11.81%\n",
      "\n",
      "test_label:23.20, test_predict:21.67, RMAE:20.67%\n",
      "\n",
      "test_label:8.10, test_predict:13.58, RMAE:12.58%\n",
      "\n",
      "test_label:5.60, test_predict:6.60, RMAE:5.60%\n",
      "\n",
      "test_label:21.70, test_predict:24.91, RMAE:23.91%\n",
      "\n",
      "test_label:29.60, test_predict:30.93, RMAE:29.93%\n",
      "\n",
      "test_label:19.60, test_predict:24.95, RMAE:23.95%\n",
      "\n",
      "test_label:7.00, test_predict:12.77, RMAE:11.77%\n",
      "\n",
      "test_label:26.40, test_predict:25.53, RMAE:24.53%\n",
      "\n",
      "test_label:18.90, test_predict:18.50, RMAE:17.50%\n",
      "\n",
      "test_label:20.90, test_predict:18.89, RMAE:17.89%\n",
      "\n",
      "test_label:28.10, test_predict:23.05, RMAE:22.05%\n",
      "\n",
      "test_label:35.40, test_predict:34.99, RMAE:33.99%\n",
      "\n",
      "test_label:10.20, test_predict:9.21, RMAE:8.21%\n",
      "\n",
      "test_label:24.30, test_predict:20.47, RMAE:19.47%\n",
      "\n",
      "test_label:43.10, test_predict:37.04, RMAE:36.04%\n",
      "\n",
      "test_label:17.60, test_predict:14.70, RMAE:13.70%\n",
      "\n",
      "test_label:15.40, test_predict:12.68, RMAE:11.68%\n",
      "\n",
      "test_label:16.20, test_predict:16.73, RMAE:15.73%\n",
      "\n",
      "test_label:27.10, test_predict:20.23, RMAE:19.23%\n",
      "\n",
      "test_label:21.40, test_predict:22.13, RMAE:21.13%\n",
      "\n",
      "test_label:21.50, test_predict:20.65, RMAE:19.65%\n",
      "\n",
      "test_label:22.40, test_predict:19.99, RMAE:18.99%\n",
      "\n",
      "test_label:25.00, test_predict:31.25, RMAE:30.25%\n",
      "\n",
      "test_label:16.60, test_predict:20.08, RMAE:19.08%\n",
      "\n",
      "test_label:18.60, test_predict:17.20, RMAE:16.20%\n",
      "\n",
      "test_label:22.00, test_predict:26.06, RMAE:25.06%\n",
      "\n",
      "test_label:42.80, test_predict:43.99, RMAE:42.99%\n",
      "\n",
      "test_label:35.10, test_predict:35.21, RMAE:34.21%\n",
      "\n",
      "test_label:21.50, test_predict:18.22, RMAE:17.22%\n",
      "\n",
      "test_label:36.00, test_predict:34.48, RMAE:33.48%\n",
      "\n",
      "test_label:21.90, test_predict:47.75, RMAE:46.75%\n",
      "\n",
      "test_label:24.10, test_predict:25.33, RMAE:24.33%\n",
      "\n",
      "test_label:50.00, test_predict:46.92, RMAE:45.92%\n",
      "\n",
      "test_label:26.70, test_predict:29.83, RMAE:28.83%\n",
      "\n",
      "test_label:25.00, test_predict:20.59, RMAE:19.59%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(test_data, test_labels): \n",
    "    predict = model.predict(i.reshape(1, -1))\n",
    "#     print(f\"test_data:\\n{i.reshape(1, -1)}\")\n",
    "    print(f\"test_label:{j:.2f}, test_predict:{predict[0][0]:.2f}, RMAE:{abs((predict[0][0]-j/j)):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
